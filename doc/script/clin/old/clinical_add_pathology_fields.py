import pandas as pd
import json
import re
from collections import Counter

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
tsv_path = "/Users/mmatsuda/Desktop/clinical_from_Project/ALLæœ€çµ‚ç¢ºèª/clin/clinical.cohort.2025-06-19/clinical.tsv"
json_path = "/Users/mmatsuda/Desktop/clinical_from_Project/ALLæœ€çµ‚ç¢ºèª/clin/clinical.cohort.2025-06-19.json"

output_path = "/Users/mmatsuda/Desktop/clinical_from_Project/ALLæœ€çµ‚ç¢ºèª/clin/clinical_expanded_final.tsv"

# è¿½åŠ ã—ãŸã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
additional_fields = [
    "diagnoses[0].pathology_details[0].anaplasia_present",
    "diagnoses[0].pathology_details[0].anaplasia_present_type",
    "diagnoses[0].pathology_details[0].breslow_thickness",
    "diagnoses[0].pathology_details[0].circumferential_resection_margin",
    "diagnoses[0].pathology_details[0].greatest_tumor_dimension",
    "diagnoses[0].pathology_details[0].gross_tumor_weight",
    "diagnoses[0].pathology_details[0].largest_extrapelvic_peritoneal_focus",
    "diagnoses[0].pathology_details[0].lymph_node_involved_site",
    "diagnoses[0].pathology_details[0].lymph_nodes_positive",
    "diagnoses[0].pathology_details[0].lymph_nodes_tested",
    "diagnoses[0].pathology_details[0].lymphatic_invasion_present",
    "diagnoses[0].pathology_details[0].non_nodal_regional_disease",
    "diagnoses[0].pathology_details[0].non_nodal_tumor_deposits",
    "diagnoses[0].pathology_details[0].percent_tumor_invasion",
    "diagnoses[0].pathology_details[0].perineural_invasion_present",
    "diagnoses[0].pathology_details[0].peripancreatic_lymph_nodes_positive",
    "diagnoses[0].pathology_details[0].peripancreatic_lymph_nodes_tested",
    "diagnoses[0].pathology_details[0].transglottic_extension",
    "diagnoses[0].pathology_details[0].tumor_largest_dimension_diameter",
    "diagnoses[0].pathology_details[0].vascular_invasion_present",
    "diagnoses[0].pathology_details[0].vascular_invasion_type"
]

# --- TSVèª­ã¿è¾¼ã¿ ---
df_tsv = pd.read_csv(tsv_path, sep='\t')

# åˆ—åã‹ã‚‰ 'cases.' ã‚’é™¤å»
df_tsv.columns = [col.replace("cases.", "") for col in df_tsv.columns]

# JSONèª­ã¿è¾¼ã¿
with open(json_path, 'r', encoding='utf-8') as f:
    json_data = json.load(f)


# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
def field_path_exists(entry, field):
    try:
        val = entry
        tokens = re.findall(r"[^\.\[\]]+", field)
        for token in tokens:
            if isinstance(val, list) and token.isdigit():
                val = val[int(token)]
            elif isinstance(val, dict):
                val = val[token] # â† .get() ã§ã¯ãªã [] ã«ã™ã‚‹ã“ã¨ã§ KeyError ã‚’ç™ºç”Ÿã•ã›ã‚‹
            else:
                return False
        return True # Noneã®ã¾ã¾è¿”ã™ã“ã¨ã§ç©ºæ¬„ã«ãªã‚‹
    except (KeyError, IndexError, TypeError, ValueError):
        return False


# å€¤ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ï¼ˆNone ã§ã‚‚ãã®ã¾ã¾è¿”ã™ï¼‰   
def extract_field(entry, field):
    try:
        val = entry
        tokens = re.findall(r"[^\.\[\]]+", field)
        for token in tokens:
            if isinstance(val, list) and token.isdigit():
                val = val[int(token)]
            elif isinstance(val, dict):
                val = val.get(token)
            else:
                return None
        return val
    except (KeyError, IndexError, TypeError, ValueError):
        return None

# --- è¿½åŠ å¯¾è±¡ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒJSONã«å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ ---
existing_fields = []
for field in additional_fields: #è¿½åŠ ã—ãŸã„å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å¯¾ã—ã¦
    for entry in json_data: #å…¨JSONç—‡ä¾‹ã«å¯¾ã—ã¦
        if field_path_exists(entry, field) :# ãã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒ1å›ã§ã‚‚å­˜åœ¨ã—ãŸã‚‰
            existing_fields.append(field)  # ãã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡ºåŠ›å¯¾è±¡ã«åŠ ãˆã‚‹
            break #ç„¡é§„ãªæ¢ç´¢ã‚’é¿ã‘ã¦æ¬¡ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¸



# JSONãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
json_records = [] #1ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼1è¡Œã®ã‚·ãƒ³ãƒ—ãƒ«ãªè¾æ›¸ã«å¤‰æ›ã—ã¦ã„ããŸã‚ã®ãƒªã‚¹ãƒˆ
for entry in json_data: #JSONã®å„ç—‡ä¾‹ï¼ˆæ‚£è€…1äººåˆ†ï¼‰ã«å¯¾ã—ã¦å‡¦ç†
    record = {"case_id": entry.get("case_id")} # ğŸ”½ ã“ã“ã§ case_id ã‚’è¿½åŠ 
    for field in existing_fields: #å®Ÿåœ¨ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€ã ã‘ã‚’å¯¾è±¡ã«å€¤ã‚’æŠ½å‡º
        record[field] = extract_field(entry, field)
    json_records.append(record)

df_json = pd.DataFrame(json_records)


# --- è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã‚½ãƒ¼ãƒˆ ---
existing_fields_sorted = sorted(existing_fields)

# --- TSVã®å¾Œã‚ã«è¿½åŠ ï¼ˆcase_idåˆ—ã¯äºŒé‡ã«ã—ãªã„ï¼‰ ---
cols_before = list(df_tsv.columns)
cols_after = [f for f in existing_fields_sorted if f not in cols_before]
df_merged = pd.merge(df_tsv, df_json, on="case_id", how="left")

# --- åˆ—é †ã‚’åˆ¶å¾¡: å…ƒTSVã®åˆ— + è¿½åŠ ã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åˆ— ---
df_merged = df_merged[cols_before + cols_after]

# --- ã‚«ãƒ©ãƒ åã®å¤‰æ› ---
original_columns = df_merged.columns.tolist()
simple_names = [f"clin_{col.split('.')[-1]}" for col in original_columns] # å…ƒã®åˆ—åã‚’.ã§åŒºåˆ‡ã‚Šã€æœ€å¾Œã ã‘å–ã‚Šå‡ºã—ã¦ã€clin_ä»˜åŠ 
name_counts = Counter(simple_names) #ç°¡ç•¥åŒ–ã—ãŸåå‰ã®é‡è¤‡ãŒãªã„ã‹
final_names = []
for orig, simple in zip(original_columns, simple_names): 
    if name_counts[simple] > 1: # é‡è¤‡ãŒã‚ã‚Œã°å…ƒã®åˆ—åã‚’_ã§ç¹‹ã’ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        safe_name = f"clin_{orig.replace('.', '_')}"
        final_names.append(safe_name)
    else:
        final_names.append(simple) # é‡è¤‡ãªã‘ã‚Œã°ç°¡ç•¥åã®ã¾ã¾
df_merged.columns = final_names

# --- "'--" ã‚’ç©ºæ–‡å­—ã«ç½®æ› ---
df_merged = df_merged.replace("'--", '', regex=False)
df_merged = df_merged.copy()

# --- ãƒ¦ãƒ‹ãƒ¼ã‚¯IDåˆ—ã®è¿½åŠ  ---
case_id_col = "clin_case_id"
treatment_col = "clin_treatment_type"
treatment_map = {
    "Pharmaceutical Therapy, NOS": "Phar",
    "Radiation Therapy, NOS": "Radi"
}
def create_unique_id(row):
    treatment = treatment_map.get(row.get(treatment_col, ''), row.get(treatment_col, ''))
    return f"{row.get(case_id_col, '')}__{treatment}"
df_merged["clin_case_id___hoge"] = df_merged.apply(create_unique_id, axis=1)

# --- å‡ºåŠ› ---
df_merged.to_csv(output_path, sep='\t', index=False)
print(f"âœ… Done. {len(cols_after)} fields added, columns renamed, '--' replaced, and unique ID column appended.")
